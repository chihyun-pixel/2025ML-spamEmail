{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d73a532-119e-42e7-84aa-1efe04a0b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/anaconda3/lib/python3.12/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from wordcloud) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c448775-c3e0-4517-9643-372401abade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55db658-919f-410c-a9b3-a947d40b089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import joblib\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    vectorizer = joblib.load(\"models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "    model_files = glob.glob(\"models/*.pkl\")\n",
    "    model_files = [m for m in model_files if \"vectorizer\" not in m]\n",
    "\n",
    "    if not model_files:\n",
    "        st.error(\"âš ï¸ æ²’æœ‰æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆï¼Œè«‹å…ˆåŸ·è¡Œ train_pipeline.pyã€‚\")\n",
    "        st.stop()\n",
    "\n",
    "    model_path = model_files[0]\n",
    "    st.info(f\"âœ… å·²è¼‰å…¥æ¨¡å‹ï¼š{model_path}\")\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    return vectorizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "547caaf8-78ce-40f9-a2f4-5b4f0411cd21",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/Logistic_Regression.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Logistic_Regression.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# æ ¹æ“šä½ æœ€ä½³æ¨¡å‹èª¿æ•´åç¨±\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer, model\n\u001b[0;32m---> 28\u001b[0m vectorizer, model \u001b[38;5;241m=\u001b[39m load_models()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ===========================\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 3. ä½¿ç”¨è€…è¼¸å…¥å€\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ===========================\u001b[39;00m\n\u001b[1;32m     33\u001b[0m st\u001b[38;5;241m.\u001b[39msubheader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ æ¸¬è©¦éƒµä»¶å…§å®¹\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/streamlit/runtime/caching/cache_utils.py:168\u001b[0m, in \u001b[0;36mmake_cached_func_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(info\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/streamlit/runtime/caching/cache_utils.py:197\u001b[0m, in \u001b[0;36mCachedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mshow_spinner, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m spinner(message, _cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create_cached_value(args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/streamlit/runtime/caching/cache_utils.py:224\u001b[0m, in \u001b[0;36mCachedFunc._get_or_create_cached_value\u001b[0;34m(self, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CacheKeyNotFoundError:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/streamlit/runtime/caching/cache_utils.py:280\u001b[0m, in \u001b[0;36mCachedFunc._handle_cache_miss\u001b[0;34m(self, cache, value_key, func_args, func_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# We acquired the lock before any other thread. Compute the value!\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mcached_message_replay_ctx\u001b[38;5;241m.\u001b[39mcalling_cached_function(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mallow_widgets\n\u001b[1;32m    279\u001b[0m ):\n\u001b[0;32m--> 280\u001b[0m     computed_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# We've computed our value, and now we need to write it back to the cache\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# along with any \"replay messages\" that were generated during value computation.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mcached_message_replay_ctx\u001b[38;5;241m.\u001b[39m_most_recent_messages\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@st\u001b[39m\u001b[38;5;241m.\u001b[39mcache_resource\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_models\u001b[39m():\n\u001b[1;32m     24\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/tfidf_vectorizer.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Logistic_Regression.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# æ ¹æ“šä½ æœ€ä½³æ¨¡å‹èª¿æ•´åç¨±\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer, model\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/Logistic_Regression.pkl'"
     ]
    }
   ],
   "source": [
    "# 03_streamlit_app.py (é€²éšç‰ˆ)\n",
    "\n",
    "import streamlit as st\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# ===========================\n",
    "# 1. é é¢è¨­å®š\n",
    "# ===========================\n",
    "st.set_page_config(page_title=\"Spam Email Classifier\", page_icon=\"ğŸ“§\", layout=\"wide\")\n",
    "\n",
    "st.title(\"ğŸ“§ Spam Email Classifier\")\n",
    "st.caption(\"ä¸€å€‹ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’å»ºç«‹çš„åƒåœ¾éƒµä»¶åˆ†é¡å™¨ | 2025 ML Project by Beck Lin\")\n",
    "\n",
    "# ===========================\n",
    "# 2. è¼‰å…¥æ¨¡å‹èˆ‡å‘é‡å™¨\n",
    "# ===========================\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    vectorizer = joblib.load(\"models/tfidf_vectorizer.pkl\")\n",
    "    model = joblib.load(\"models/Logistic_Regression.pkl\")  # æ ¹æ“šä½ æœ€ä½³æ¨¡å‹èª¿æ•´åç¨±\n",
    "    return vectorizer, model\n",
    "\n",
    "vectorizer, model = load_models()\n",
    "\n",
    "# ===========================\n",
    "# 3. ä½¿ç”¨è€…è¼¸å…¥å€\n",
    "# ===========================\n",
    "st.subheader(\"ğŸ“ æ¸¬è©¦éƒµä»¶å…§å®¹\")\n",
    "user_input = st.text_area(\n",
    "    \"è«‹è¼¸å…¥éƒµä»¶å…§å®¹ï¼š\",\n",
    "    height=150,\n",
    "    placeholder=\"ä¾‹å¦‚ï¼šCongratulations! You've won a $1000 gift card. Click here to claim...\",\n",
    ")\n",
    "\n",
    "col1, col2 = st.columns([1, 2])\n",
    "\n",
    "if col1.button(\"ğŸ” é–‹å§‹åˆ†æ\"):\n",
    "    if user_input.strip() == \"\":\n",
    "        st.warning(\"è«‹å…ˆè¼¸å…¥éƒµä»¶å…§å®¹ã€‚\")\n",
    "    else:\n",
    "        X_input = vectorizer.transform([user_input])\n",
    "        pred = model.predict(X_input)[0]\n",
    "        try:\n",
    "            proba = model.predict_proba(X_input)[0][1]\n",
    "        except:\n",
    "            proba = None\n",
    "\n",
    "        if pred == 1:\n",
    "            col2.error(\"ğŸš¨ é æ¸¬çµæœï¼š**Spam (åƒåœ¾éƒµä»¶)**\")\n",
    "        else:\n",
    "            col2.success(\"âœ… é æ¸¬çµæœï¼š**Not Spam (æ­£å¸¸éƒµä»¶)**\")\n",
    "\n",
    "        if proba is not None:\n",
    "            col2.metric(\"Spam æ©Ÿç‡\", f\"{proba*100:.2f}%\")\n",
    "            st.progress(float(proba))\n",
    "\n",
    "# ===========================\n",
    "# 4. æ¨¡å‹æ•ˆèƒ½æ‘˜è¦\n",
    "# ===========================\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"ğŸ“Š æ¨¡å‹æ•ˆèƒ½æ‘˜è¦\")\n",
    "\n",
    "try:\n",
    "    # å˜—è©¦å¾ pipeline ç”¢ç”Ÿçš„çµæœè¡¨è¼‰å…¥ (è‹¥æœ‰)\n",
    "    df_metrics = pd.read_csv(\"data/model_results.csv\")\n",
    "    st.dataframe(df_metrics.style.highlight_max(subset=[\"F1\"], color=\"lightgreen\"))\n",
    "except:\n",
    "    # è‹¥æ²’æœ‰çµæœæª”ï¼Œé¡¯ç¤ºæ‰‹å‹•æ‘˜è¦\n",
    "    st.info(\"\"\"\n",
    "    **æ¨¡å‹æ‘˜è¦ (æ‰‹å‹•è¼¸å…¥)**\n",
    "    - Logistic Regressionï¼šAccuracy 98%ã€F1 0.97\n",
    "    - Naive Bayesï¼šAccuracy 97%ã€F1 0.94\n",
    "    - SVMï¼šAccuracy 97%ã€F1 0.96\n",
    "    \"\"\")\n",
    "\n",
    "# ===========================\n",
    "# 5. è©é›²å¯è¦–åŒ–\n",
    "# ===========================\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"â˜ï¸ è©é›²è¦–è¦ºåŒ–\")\n",
    "\n",
    "try:\n",
    "    data = joblib.load(\"data/processed_spam_data.pkl\")\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    # å› ç‚ºé€™è£¡æ²’æœ‰åŸå§‹æ–‡å­—è³‡æ–™ï¼Œæˆ‘å€‘ç”¨ç¤ºæ„è³‡æ–™\n",
    "    spam_texts = [\n",
    "        \"free winner cash prize money offer congratulations click claim now\",\n",
    "        \"you have won lottery gift card free coupon claim reward now\",\n",
    "    ]\n",
    "    ham_texts = [\n",
    "        \"see you at lunch tomorrow meeting scheduled at 3pm\",\n",
    "        \"please find attached the report for this week project update\",\n",
    "    ]\n",
    "\n",
    "    spam_wc = WordCloud(width=500, height=300, background_color=\"white\").generate(\" \".join(spam_texts))\n",
    "    ham_wc = WordCloud(width=500, height=300, background_color=\"white\").generate(\" \".join(ham_texts))\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.image(spam_wc.to_array(), caption=\"ğŸš¨ Spam å¸¸è¦‹è©\")\n",
    "    with col2:\n",
    "        st.image(ham_wc.to_array(), caption=\"âœ… Ham å¸¸è¦‹è©\")\n",
    "\n",
    "except Exception as e:\n",
    "    st.warning(\"ç„¡æ³•ç”¢ç”Ÿè©é›²ï¼ˆå¯èƒ½ç¼ºå°‘æ–‡å­—è³‡æ–™ï¼‰\")\n",
    "\n",
    "# ===========================\n",
    "# 6. ç°¡å–®çµ±è¨ˆåœ–è¡¨\n",
    "# ===========================\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"ğŸ“ˆ Spam vs Ham çµ±è¨ˆåˆ†æ\")\n",
    "\n",
    "spam_count = 747\n",
    "ham_count = 4827\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# åœ“é¤…åœ–\n",
    "ax[0].pie([ham_count, spam_count], labels=[\"Ham\", \"Spam\"], autopct=\"%1.1f%%\", colors=[\"#4CAF50\", \"#F44336\"])\n",
    "ax[0].set_title(\"è³‡æ–™é›†æ¯”ä¾‹\")\n",
    "\n",
    "# é•·åº¦åˆ†ä½ˆï¼ˆæ¨¡æ“¬ï¼‰\n",
    "np.random.seed(42)\n",
    "ham_lengths = np.random.normal(80, 20, 200)\n",
    "spam_lengths = np.random.normal(120, 25, 200)\n",
    "sns.kdeplot(ham_lengths, ax=ax[1], label=\"Ham\")\n",
    "sns.kdeplot(spam_lengths, ax=ax[1], label=\"Spam\", color=\"red\")\n",
    "ax[1].set_title(\"éƒµä»¶é•·åº¦åˆ†ä½ˆ\")\n",
    "ax[1].set_xlabel(\"å­—æ•¸\")\n",
    "ax[1].legend()\n",
    "\n",
    "st.pyplot(fig)\n",
    "\n",
    "# ===========================\n",
    "# 7. é—œæ–¼æ¨¡å‹èªªæ˜\n",
    "# ===========================\n",
    "st.markdown(\"---\")\n",
    "with st.expander(\"ğŸ“˜ é—œæ–¼æ­¤æ¨¡å‹\"):\n",
    "    st.write(\"\"\"\n",
    "    - æ¨¡å‹ä½¿ç”¨ **TF-IDF å‘é‡åŒ–** + **Logistic Regression**\n",
    "    - è¨“ç·´è³‡æ–™ä¾†è‡ª SMS Spam Collection Dataset\n",
    "    - è©•ä¼°æŒ‡æ¨™ï¼šAccuracyã€Precisionã€Recallã€F1-score\n",
    "    - æ–¼ Streamlit Cloud éƒ¨ç½²ï¼Œå¯å³æ™‚é æ¸¬éƒµä»¶å…§å®¹æ˜¯å¦ç‚ºåƒåœ¾éƒµä»¶ã€‚\n",
    "    \"\"\")\n",
    "\n",
    "st.markdown(\"â€”\")\n",
    "st.caption(\"ğŸ§  Created by Beck Lin | 2025 Machine Learning Project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c712b728-5448-4d79-acea-722f9a2ae685",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (120509875.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    streamlit run 03_streamlit_app.py\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "streamlit run 03_streamlit_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c0594-4858-473f-808a-8f4189c25c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
