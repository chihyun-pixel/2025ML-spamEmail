"""
train_pipeline.py
æ•´åˆç‰ˆè¨“ç·´è…³æœ¬ï¼š
è‡ªå‹•å®Œæˆè³‡æ–™ä¸‹è¼‰ã€é è™•ç†ã€æ¨¡å‹è¨“ç·´ã€æˆæœè¦–è¦ºåŒ–èˆ‡çµæœè¼¸å‡º
"""

import os
import re
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix
)

# ===============================
# 0. NLTK åˆå§‹åŒ–
# ===============================
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)

# ===============================
# 1. ä¸‹è¼‰è³‡æ–™
# ===============================
print("ğŸ“¥ è¼‰å…¥è³‡æ–™ä¸­...")
url = "https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv"
df = pd.read_csv(url, sep='\t', header=None, names=['label', 'message'])
print(f"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™ï¼Œå…± {df.shape[0]} ç­†æ¨£æœ¬")

# ===============================
# 2. æ–‡å­—æ¸…ç†
# ===============================
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = re.sub('[^a-zA-Z]', ' ', text)
    text = text.lower()
    words = text.split()
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
    return ' '.join(words)

print("ğŸ§¹ æ¸…ç†æ–‡å­—ä¸­...")
df['clean_text'] = df['message'].apply(clean_text)

# ===============================
# 3. å‘é‡åŒ–
# ===============================
vectorizer = TfidfVectorizer(max_features=3000)
X = vectorizer.fit_transform(df['clean_text']).toarray()
y = np.where(df['label'] == 'spam', 1, 0)

print("ğŸ”  å‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾µç¶­åº¦ï¼š", X.shape)

# ===============================
# 4. è³‡æ–™åˆ†å‰²
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ===============================
# 5. æ¨¡å‹è¨“ç·´
# ===============================
models = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Linear SVM": LinearSVC(),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42)
}

results = []

print("ğŸ¤– é–‹å§‹æ¨¡å‹è¨“ç·´...\n")
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    try:
        y_proba = model.predict_proba(X_test)[:, 1]
        roc = roc_auc_score(y_test, y_proba)
    except:
        roc = np.nan

    results.append([name, acc, prec, rec, f1, roc])
    print(f"{name}: F1={f1:.4f}, Accuracy={acc:.4f}")

# ===============================
# 6. é¸å‡ºæœ€ä½³æ¨¡å‹
# ===============================
results_df = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision", "Recall", "F1", "ROC_AUC"])
best_row = results_df.loc[results_df['F1'].idxmax()]
best_model_name = best_row['Model']
best_model = models[best_model_name]

print("\nğŸ† æœ€ä½³æ¨¡å‹:", best_model_name)
print(results_df)

# ===============================
# 7. å„²å­˜æˆæœ
# ===============================
os.makedirs("data", exist_ok=True)
os.makedirs("models", exist_ok=True)

# å„²å­˜è™•ç†å¾Œè³‡æ–™ã€æ¨¡å‹ã€å‘é‡å™¨
joblib.dump((X_train, X_test, y_train, y_test), "data/processed_spam_data.pkl")
joblib.dump(vectorizer, "models/tfidf_vectorizer.pkl")
joblib.dump(best_model, f"models/{best_model_name.replace(' ', '_')}.pkl")

# å„²å­˜æ¨¡å‹çµæœè¡¨æ ¼ä¾› Streamlit ä½¿ç”¨
results_df.to_csv("data/model_results.csv", index=False)

print("\nğŸ’¾ æ¨¡å‹èˆ‡è³‡æ–™å·²å„²å­˜å®Œæˆï¼")
print("ğŸ“ data/processed_spam_data.pkl")
print("ğŸ“ data/model_results.csv")
print("ğŸ“ models/tfidf_vectorizer.pkl")
print(f"ğŸ“ models/{best_model_name.replace(' ', '_')}.pkl")

# ===============================
# 8. æ··æ·†çŸ©é™£è¦–è¦ºåŒ–
# ===============================
cm = confusion_matrix(y_test, best_model.predict(X_test))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f"{best_model_name} Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("\nâœ… è¨“ç·´æµç¨‹å®Œæˆï¼å¯ç›´æ¥ç”¨æ–¼ Streamlit Demoã€‚")
